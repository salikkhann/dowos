# Decision: RAG & AI Tutor Architecture

**Date:** 2026-02-04 | **Status:** LOCKED | **Owner:** Day 3 decision sprint

---

## 1. What was already locked (do not re-debate)

From `FINAL_LOCKED_DECISIONS.md`, `00_DISCOVERY_RESOLVED.md`, and `model-selection.md`:

| Item | Decision |
|---|---|
| Tier 1 â€” interactive LLM | Gemini 2.5 Flash (Search grounding, streaming) |
| Tier 2 â€” reasoning LLM | DeepSeek V3.2 Reasoner (fires on COMPLEX queries only) |
| Tier 3 â€” batch / background | Gemini 2.5 Flash-Lite (ingestion, distillation, classifiers) |
| Complexity router | Flash-Lite classifier emits SIMPLE \| COMPLEX; see `model-selection.md` Â§3.2 |
| Embedding vendor + dims | `gemini-embedding-001`, 768 dims (MRL), asymmetric task types |
| Vector store | pgvector on Supabase (HNSW) |
| Memory tiers | Short-term (session, last 10 msgs) + Long-term (`student_memory` with weak/strong signals; see `model-selection.md` Â§4) |
| Knowledge corpus | 25 medical textbooks + Dow slides + **past-paper question banks** (all PDFs via admin dashboard) |
| High-yield layer | Past papers â†’ Flash-Lite batch extracts questions + topics â†’ scored by frequency â†’ stored in `high_yield_topics`; injected into every tutor prompt |
| Rate limits | Soft 2 msgs/day (+5 s delay), Hard 4 msgs/day (blocked), Pro = unlimited |
| Tutor modes | Chat (free-form) + Tutor (structured drill) |
| Pro personalisation | Spaced-repetition engine, adaptive difficulty, weekly study plans, exam-readiness scores â€” all Pro-only; see Â§8 |
| Cost target | ~$41 / month at 500 DAU (full breakdown in `model-selection.md` Â§5) |

---

## 2. Open questions this doc resolves

The PRD flagged five items for Day 3:

1. Chunking strategy â€” fixed vs sentence vs semantic
2. Embedding model variant and output dimensions
3. Retrieval mode â€” dense-only vs hybrid (sparse + dense)
4. Re-ranking and context-window budget per query
5. Medical terminology density and Q&A extraction from PDFs

**Additional requirements added by product owner (Feb 4):**

6. The tutor must also be able to cite live web / Google Search results, not only the static textbook corpus.
7. The experience must feel seamless and conversational â€” low latency, streaming, no robotic pauses.
8. Past-paper questions will be uploaded alongside textbooks. The system must mine them to surface **high-yield topics** (frequently tested) and use that signal when teaching â€” so the tutor prioritises what actually shows up on exams.
9. **Pro users get a deeper personalisation layer:** spaced-repetition scheduling, adaptive difficulty, weekly study-plan generation, and exam-readiness scoring â€” all driven by the same student signals the memory system already collects. This is the core value prop of the PKR 3 000 / yr subscription.

---

## 3. Options evaluated

### Option A â€” Gemini File Search (fully managed RAG)

Google's built-in managed RAG: upload files, Gemini handles chunking, embedding, retrieval, and injection automatically. Zero infrastructure.

**Pros:** Zero ops, free storage, automatic chunking.
**Cons:** Black box â€” no access to chunk boundaries, no custom metadata filtering (module, subject, batch_year, difficulty), no hybrid BM25 layer, no way to weight user-specific context or inject short-term memory alongside retrieved chunks. Unsuitable for a tutor that must filter by a student's current module and cite specific Dow slides.

### Option B â€” Hybrid pgvector + gemini-embedding-001 + Google Search grounding âœ“ CHOSEN

Custom ingestion pipeline stores textbook/slide chunks in pgvector with rich metadata. Retrieval uses hybrid search (dense vector + BM25 keyword). A separate Google Search grounding call runs in parallel on every query so the tutor can also cite live web sources. Gemini Flash streams the final answer, interleaving textbook citations and web citations.

**Pros:** Full control over chunking, metadata, and retrieval weights. Hybrid search catches exact medical terms BM25 misses semantically. Web grounding adds live citation without extra infra. Stays entirely on Supabase â€” no new services. pgvector benchmarks show 1185 % more QPS than Pinecone at lower cost for 1 M vectors (Supabase blog, Jan 2025). Gemini embedding-001 tops MTEB multilingual leaderboard and supports MRL truncation.
**Cons:** More code to write than File Search. Requires an ingestion pipeline triggered on admin upload.

### Option C â€” Separate vector DB (Qdrant or Pinecone) + Gemini embeddings

Move vectors out of Supabase into a dedicated store.

**Pros:** Specialist stores scale to billions of vectors.
**Cons:** Our corpus is ~25 textbooks + slides â€” conservatively 2â€“5 M chunks. pgvector handles that comfortably. Adding a second database doubles operational surface, adds a sync problem (metadata lives in Postgres, vectors live elsewhere), and introduces network latency for every retrieval hop. No cost or performance advantage at this scale.

---

## 4. Chosen architecture â€” detailed spec

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Student sends message                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                â”Œâ”€â”€â”€ Complexity Router â”€â”€â”€â”
                â”‚  Flash-Lite classifier  â”‚
                â”‚  emits SIMPLE | COMPLEX â”‚
                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          SIMPLE       â”‚         â”‚  COMPLEX
                       â–¼         â–¼
              â”Œâ”€â”€ Tier 1 â”€â”€â”  â”Œâ”€â”€ Tier 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Flash      â”‚  â”‚ DeepSeek R1        â”‚
              â”‚ (grounding)â”‚  â”‚ (deep reasoning)   â”‚
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚                 â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â–¼               â–¼              â–¼              â–¼               â–¼
â”Œâ”€ Textbook RAG â”€â” â”Œâ”€ Student  â”€â” â”Œâ”€ Past Paper â”€â” â”Œâ”€â”€ Google  â”€â”€â”
â”‚ 1. Meta filter â”‚ â”‚   Memory   â”‚ â”‚    Intel      â”‚ â”‚  Search     â”‚
â”‚    (module,    â”‚ â”‚ 1. Semanticâ”‚ â”‚ 1. High-yield â”‚ â”‚  Grounding  â”‚
â”‚     year)      â”‚ â”‚    search  â”‚ â”‚    topics for â”‚ â”‚  (Tier 1    â”‚
â”‚ 2. Dense top-20â”‚ â”‚ 2. Strong  â”‚ â”‚    this moduleâ”‚ â”‚   only;     â”‚
â”‚ 3. Sparse top-20â”‚ â”‚    first   â”‚ â”‚ 2. Frequency  â”‚ â”‚   auto-     â”‚
â”‚ 4. RRF merge  â”‚ â”‚ 3. Upcomingâ”‚ â”‚    scores     â”‚ â”‚   gated)    â”‚
â”‚ 5. Re-rank    â”‚ â”‚    viva    â”‚ â”‚ 3. Past Q&A   â”‚ â”‚  â†’ chunks   â”‚
â”‚ 6. MMR dedup  â”‚ â”‚    context â”‚ â”‚    examples   â”‚ â”‚  + cites    â”‚
â”‚ 7. Trim budgetâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  All retrieved context assembled into a single prompt:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  System prompt                                             â”‚
  â”‚    â€¢ persona (Chat vs Tutor tone)                          â”‚
  â”‚    â€¢ learning_style + explanation_depth                    â”‚
  â”‚  Short-term memory              â‰¤ 1 500 tokens            â”‚
  â”‚  Student memory signals         â‰¤ 1 000 tokens            â”‚
  â”‚  High-yield / past-paper intel  â‰¤   800 tokens            â”‚
  â”‚  Textbook chunks [T1â€¦T5]       â‰¤ 4 200 tokens            â”‚
  â”‚  Citation instructions                                     â”‚
  â”‚    â€¢ [T n] = textbook  [W n] = web  [P n] = past paper   â”‚
  â”‚  google_search tool (Tier 1 only)                          â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
  â”‚  Total context budget           â‰¤ 8 000 tokens            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
              SSE stream â†’ client (progressive render)
```

### 4.1 Embedding model

| Parameter | Value | Rationale |
|---|---|---|
| Model | `gemini-embedding-001` | Tops MTEB multilingual; task-type aware; single vendor with Flash |
| Output dimensions | **768** (MRL truncation) | Half the storage of 1536 with <2 % quality loss per Google benchmarks |
| Task type â€” documents | `RETRIEVAL_DOCUMENT` | Asymmetric: doc embeddings optimised differently from query embeddings |
| Task type â€” queries | `RETRIEVAL_QUERY` | Gemini enforces this split; critical for recall |
| Max input | 2 048 tokens | Chunks must stay under this limit |

### 4.2 Chunking strategy â€” semantic with structure preservation + parent-document indexing

**Design principles (from `rag-engineer` skill):** chunk by meaning, not arbitrary token counts. Preserve document structure. Include overlap for context continuity. Add rich metadata for filtering. Never embed everything blindly â€” each chunk must earn its place.

| Rule | Detail |
|---|---|
| Primary split | PDF â†’ extract text (`pdf.js` server-side). Split on section headers first (H1 â†’ H2 â†’ H3), then on paragraph boundaries. |
| Secondary split | If a section exceeds **400 tokens**, apply recursive character splitting with separators `["\n\n", "\n", ". ", " "]`. Never split mid-sentence. |
| Chunk size target | **300â€“400 tokens** (~1 200â€“1 600 chars). Stays well under the 2 048 token embedding limit; each chunk stays focused on one concept. |
| Overlap | **50 tokens** (~15 % of chunk) between adjacent chunks. Prevents context loss at boundaries â€” a sharp edge flagged by `rag-engineer`. |
| Parent-document indexing | Each chunk stores a `parent_section_id` pointing to the full section text (stored separately, not embedded). When a child chunk is retrieved, the full parent section is available for contextual compression â€” the model sees the chunk in its original surrounding context before deciding what to include. This follows the Parent Document Retriever pattern from `rag-implementation`. |
| Metadata attached | `source_id`, `book_title`, `chapter`, `section`, `page_number`, `module_tag` (admin-assigned), `subject_tag`, `subtopic_tag`, `batch_relevance[]`, `chunk_type` |
| Q&A extraction | During ingestion, run **Flash-Lite batch** (not Flash) per section: *"Extract up to 3 exam-style Q&A pairs from this text."* Store as sibling chunks tagged `chunk_type: qa`. These surface first for drill-mode queries. Flash-Lite is adequate for extraction; Flash is reserved for interactive calls. |
| Medical term handling | Before embedding, run a lightweight normalisation pass: expand abbreviations common in medical texts (e.g. HTN â†’ hypertension, DM â†’ diabetes mellitus). Attach the expanded form to the chunk's metadata `terms[]` field. BM25 sparse search indexes these expanded terms â€” critical for recall on medical vocabulary (sharp edge: same embedding model across content types loses domain specificity). |
| Past-paper chunking | Past-paper PDFs are chunked the same way as textbooks but every extracted question + answer is stored as its own chunk with `chunk_type: past_paper`. Metadata adds `exam_year`, `exam_type` (`annual` \| `supplementary` \| `mock`), and `topic_tags[]` (extracted by Flash-Lite). These chunks are **never** used as direct answers â€” they exist solely to (a) teach the retrieval layer what topics matter and (b) surface as example questions in Tutor drill mode. |
| High-yield scoring | After all past-paper questions for a module are ingested, a single Flash-Lite batch call receives the full list of `topic_tags[]` across every question and returns a JSON frequency map: `{ "coronary_circulation": 14, "cardiac_output": 9, â€¦ }`. This is stored in `high_yield_topics` (one row per module-subtopic). Any subtopic appearing in â‰¥ 3 past papers is flagged `is_high_yield = true`. The flag is injected into the tutor's system prompt and used to bias retrieval weights: high-yield chunks get a **1.3Ã— score multiplier** after RRF, before re-ranking. |

### 4.3 Retrieval â€” hybrid search with dedup and compression

**Design principles (from `rag-engineer` + `hybrid-search-implementation` skills):** pure semantic search misses exact medical terms; pure keyword search misses meaning. Hybrid with RRF is the proven sweet spot. First-stage results must always be re-ranked â€” using first-stage results directly is a flagged anti-pattern. Apply relevance thresholds, never cram maximum context into the prompt.

| Step | Detail |
|---|---|
| 1. Metadata pre-filter | SQL WHERE on `module_tag`, `subject_tag`, and optionally `subtopic_tag` + `batch_relevance`. Eliminates irrelevant textbooks before any vector math â€” critical for latency and precision. |
| 2. Dense retrieval | pgvector `<=>` cosine similarity, HNSW index, top-20 candidates. Embed the query with `gemini-embedding-001` using task type `RETRIEVAL_QUERY` (asymmetric from the `RETRIEVAL_DOCUMENT` used at index time). |
| 3. Sparse retrieval | PostgreSQL full-text search (`tsvector` / `ts_query`) on the `text` column **plus** the normalised `terms[]` metadata field, top-20 candidates. Catches exact drug names, anatomical structures, and expanded abbreviations that dense search misses. |
| 4. Reciprocal Rank Fusion | Merge: `score = 1 / (k + rank)`, `k = 60`. Default weight: **0.7 dense / 0.3 sparse**. For drill-mode (Tutor) queries, shift to 0.5 / 0.5 â€” exam questions rely more on exact terminology. |
| 5. Cross-encoder re-rank | Top-10 fused candidates â†’ re-ranked with `cross-encoder/ms-marco-MiniLM-L-6-v2` (hosted as a single Edge Function, warm-started). Returns top-7. This is the step that turns "pretty good" retrieval into "actually correct" retrieval. |
| 6. MMR diversity filter | Apply Maximal Marginal Relevance on the top-7: `Î» = 0.7` (relevance-heavy). Produces **top-5 diverse chunks** â€” avoids sending 5 chunks that all say the same thing. Pattern from `rag-implementation` skill. |
| 7. Contextual compression | For each of the 5 chunks, retrieve its `parent_section` text. Run a single Flash-Lite call: *"Given this question, extract only the sentences from these passages that are directly relevant."* This trims filler while keeping the answer grounded. Output replaces the raw chunks. |
| 8. Relevance threshold | Drop any chunk whose cross-encoder score falls below **0.3**. Do not pad context with irrelevant material â€” the model will hallucinate on weak context. Sharp edge from `rag-engineer`. |
| 9. Context budget | Sum token lengths of final chunks. Hard cap: **4 500 tokens** for textbook context. Remaining budget: 2 000 short-term memory + 1 500 student memory signals = 8 000 total. |

### 4.4 Google Search grounding â€” live web layer

| Parameter | Value |
|---|---|
| Tool | `google_search: GoogleSearch()` passed alongside the textbook context in the same request |
| Models | Gemini 2.5 Flash supports it natively |
| What it does | Gemini auto-decides whether a Google query would help. If yes, it executes queries, pulls `groundingChunks`, and weaves web citations into the response alongside textbook citations |
| Citation format | Response includes `groundingSupports` with segment indices â†’ map to `[W1]`, `[W2]` in the rendered UI |
| When it fires | Every message. Gemini self-gates â€” for a question like *"what is the mechanism of metformin?"* it will likely lean on textbook context; for *"latest WHO diabetes guidelines 2026"* it will lean on web results. No manual routing needed. |
| Cost note | Billed per search query the model executes (Gemini 2.5 billing: per prompt). Within free-tier limits for â‰¤ 4 msgs/day per user. Monitor via Google Cloud Console. |

### 4.5 Streaming & conversational UX

| Requirement | How |
|---|---|
| Low latency first token | Gemini 2.5 Flash median first-token latency < 500 ms. Use server-sent events (SSE) from a Next.js Route Handler to stream tokens to the client. |
| No robotic pauses | Stream text as it arrives â€” do not buffer the full response before rendering. TanStack Query `useInfiniteQuery` or a simple `ReadableStream` consumer on the client handles progressive rendering. |
| Citation anchoring | Citations appear inline as the relevant sentence streams in, not as a footnote block at the end. Frontend maps `groundingSupports` segment indices to the streamed text positions. |
| Memory continuity | Last 10 messages (short-term) are prepended to every request so the model does not repeat itself or lose thread. Stored in `chat_messages` table, cached in TanStack Query. |
| Tutor vs Chat tone | System prompt switches voice: Chat = peer/friend ("hey, good question â€”"); Tutor = structured guide ("Step 1: â€¦"). Learning style and depth from onboarding profile drive the detail level. |

### 4.6 Long-term memory â€” `student_memory` signal system

The old `user_knowledge_base` blob approach is replaced by a structured signal table (defined in `model-selection.md` Â§4). Full taxonomy, update triggers, and decay rules live there. This section covers only how memory integrates with the RAG pipeline.

**Injection into every query (budget: â‰¤ 1 500 tokens):**

1. Embed the student's query â†’ semantic search `student_memory` (HNSW, top-5).
2. Sort results: **strong signals first**, weak signals after. Within each tier, sort by `updated_at` descending (most recent first).
3. Append **upcoming-event context**: query `viva_schedules` for the next 7 days. If any upcoming viva topic overlaps with a weak or strong struggle signal, promote that signal to the top of the injection and add: *"Upcoming viva: [topic] on [date]. Current MCQ accuracy: [value]%."*
4. Append **high-yield intel** for the student's current module: pull the top-5 subtopics from `high_yield_topics` where `is_high_yield = true`, ordered by frequency descending. Include the frequency count so the model understands exam weight.
5. Serialise everything as a compact block in the system prompt. Example:
   ```
   [STUDENT CONTEXT]
   âš  Upcoming viva: Coronary Circulation â€” Feb 8. MCQ accuracy 42 % (strong).
   â€¢ Struggles with: cardiac output regulation (strong, viva 18/25)
   â€¢ Recently confused about: Frank-Starling mechanism (weak, chat)
   â€¢ Mastery: Renal physiology 88 % (strong) â€” do not re-explain basics here.

   [HIGH-YIELD â€” Module HAE001]
   ğŸ”¥ Coronary circulation        â€” appeared in 14 past papers
   ğŸ”¥ Cardiac output regulation   â€” appeared in  9 past papers
      Atrial fibrillation mgmt    â€” appeared in  5 past papers
   â†’ Prioritise these topics in explanations. If the student asks about something
     not on this list, answer fully but do not go deep unless they follow up.
   ```
6. If total exceeds 1 500 tokens (student memory + high-yield combined), drop weakest signals first. **Never drop** an upcoming-viva signal or a high-yield topic with frequency â‰¥ 5 â€” those are exam-critical.

**Post-session distillation (async, Flash-Lite batch):**

After 10 min of inactivity or navigation away, a background job:
1. Reads the session's `chat_messages`.
2. Runs Flash-Lite: *"Identify topics the student struggled with, topics they understood well, and any self-reported confusion. Return as JSON: `{struggles: [], understood: [], confused: []}`"*
3. For each `struggles[]` item: UPSERT `student_memory` with `signal_type = 'chat_struggle'`, `strength = 'weak'`.
4. For each `understood[]` item: if an existing strong struggle signal exists on that subtopic, reduce its `value` by 10 % (the student is improving). This is the decay-on-improvement rule from `model-selection.md` Â§4.6.

---

## 5. What to store and where

### Supabase tables (migration 00002 = knowledge + chat; migration 00003 = student_memory)

| Table | Migration | Purpose | Key columns |
|---|---|---|---|
| `knowledge_sources` | 00002 | Registry of uploaded PDFs / slide decks | `id`, `title`, `file_url`, `module_id`, `uploaded_by`, `status` (`pending` \| `indexed` \| `error`) |
| `knowledge_sections` | 00002 | Full-text sections (parent documents for contextual compression) | `id`, `source_id`, `section_path` (e.g. `"Ch3 > Coronary Circulation"`), `full_text`, `module_tag`, `subject_tag` |
| `knowledge_chunks` | 00002 | Pre-chunked, embedded child segments | `id`, `source_id`, `parent_section_id` (FK â†’ `knowledge_sections`), `text`, `embedding vector(768)`, `module_tag`, `subject_tag`, `subtopic_tag`, `chunk_type` (`text` \| `qa`), `terms[]` (normalised medical terms), `metadata jsonb` |
| `chat_sessions` | 00002 | One row per conversation | `id`, `user_id`, `mode` (`chat` \| `tutor`), `created_at`, `last_active_at` |
| `chat_messages` | 00002 | Individual messages with role + content | `id`, `session_id`, `role` (`user` \| `assistant`), `content`, `citations jsonb`, `tokens_used`, `created_at` |
| `past_paper_questions` | 00002 | Individual questions extracted from past-paper PDFs | `id`, `source_id`, `question_text`, `correct_answer`, `topic_tags[]`, `exam_year`, `exam_type`, `module_id`, `subject_id`, `subtopic_id`, `embedding vector(768)`, `created_at` |
| `high_yield_topics` | 00002 | Derived frequency table: which subtopics appear most in past papers | `id`, `module_id`, `subtopic_id`, `frequency` (int, count of past papers), `is_high_yield` (bool, freq â‰¥ 3), `last_recalculated` |
| `student_memory` | 00003 | Per-user, per-subtopic signal store (weak/strong) | `id`, `user_id`, `subtopic_id`, `module_id`, `signal_type`, `strength`, `value`, `summary_text`, `embedding vector(768)`, `created_at`, `updated_at` |

### Supabase indexes

```sql
-- knowledge_chunks
CREATE INDEX idx_chunks_embedding   ON knowledge_chunks USING hnsw (embedding vector_cosine_ops);
CREATE INDEX idx_chunks_fts         ON knowledge_chunks USING gin (
  to_tsvector('english', text || ' ' || array_to_string(terms, ' '))
);
CREATE INDEX idx_chunks_module      ON knowledge_chunks (module_tag, subject_tag);
CREATE INDEX idx_chunks_parent      ON knowledge_chunks (parent_section_id);

-- knowledge_sections (no vector index â€” retrieved by FK, not by similarity)
CREATE INDEX idx_sections_source    ON knowledge_sections (source_id);

-- past_paper_questions
CREATE INDEX idx_pp_embedding       ON past_paper_questions USING hnsw (embedding vector_cosine_ops);
CREATE INDEX idx_pp_module          ON past_paper_questions (module_id, subject_id);
CREATE INDEX idx_pp_subtopic        ON past_paper_questions (subtopic_id);

-- high_yield_topics (small table, simple btree is fine)
CREATE INDEX idx_hy_module          ON high_yield_topics (module_id, is_high_yield DESC, frequency DESC);

-- student_memory
CREATE INDEX idx_sm_user            ON student_memory (user_id);
CREATE INDEX idx_sm_subtopic        ON student_memory (user_id, subtopic_id);
CREATE INDEX idx_sm_embedding       ON student_memory USING hnsw (embedding vector_cosine_ops);
CREATE INDEX idx_sm_strength        ON student_memory (user_id, strength, updated_at DESC);
```

### Supabase RLS

| Table(s) | Policy |
|---|---|
| `knowledge_sources`, `knowledge_sections`, `knowledge_chunks`, `past_paper_questions`, `high_yield_topics` | **Read:** all authenticated users. **Insert / update / delete:** service-role only (admin ingestion pipeline). |
| `chat_sessions`, `chat_messages` | **Self-only:** `user_id = auth.uid()` on all operations. |
| `student_memory` | **Self-only read:** `user_id = auth.uid()`. **Write:** service-role only (triggers + batch jobs update signals; students never write directly). |

---

## 6. Data flow â€” ingestion (when admin uploads a PDF)

Admin tags the upload at the time of upload as one of: `textbook` | `slides` | `past_paper`. The pipeline forks accordingly.

```
Admin uploads PDF via /admin/upload  (tags: type, module, subject)
        â”‚
        â–¼
Next.js Route Handler (server)
  1. Save file to Supabase Storage (knowledge_sources bucket)
  2. Insert row into knowledge_sources (status = pending, type = tagged type)
  3. Kick off ingestion Edge Function
        â”‚
        â–¼
  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  type = textbook | slides        â”‚  type = past_paper
  â–¼                                  â–¼
â”Œâ”€ Textbook / Slides pipeline â”€â”   â”Œâ”€ Past Paper pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Extract text (pdf.js)     â”‚   â”‚ 1. Extract text (pdf.js)           â”‚
â”‚ 2. Split into sections       â”‚   â”‚ 2. Flash-Lite: "Parse this past    â”‚
â”‚    â†’ store in                â”‚   â”‚    paper. Return JSON array:        â”‚
â”‚    knowledge_sections        â”‚   â”‚    [{question, answer, topic_tags,  â”‚
â”‚ 3. Split sections into       â”‚   â”‚      exam_year, exam_type}]"        â”‚
â”‚    chunks (300-400 tok,      â”‚   â”‚ 3. For each extracted question:     â”‚
â”‚    50-tok overlap)           â”‚   â”‚    a. Embed with embedding-001      â”‚
â”‚ 4. Normalise medical terms   â”‚   â”‚       (RETRIEVAL_DOCUMENT, 768)     â”‚
â”‚    â†’ terms[] metadata        â”‚   â”‚    b. Insert into                   â”‚
â”‚ 5. For each chunk:           â”‚   â”‚       past_paper_questions          â”‚
â”‚    a. Embed (embedding-001,  â”‚   â”‚ 4. Also store each Q as a          â”‚
â”‚       RETRIEVAL_DOCUMENT)    â”‚   â”‚    knowledge_chunk (type: past_paperâ”‚
â”‚    b. Flash-Lite Q&A extract â”‚   â”‚    ) so it participates in hybrid   â”‚
â”‚       â†’ sibling qa chunks    â”‚   â”‚    retrieval during drill mode      â”‚
â”‚    c. INSERT knowledge_chunksâ”‚   â”‚ 5. Recalculate high_yield_topics   â”‚
â”‚ 6. Status â†’ indexed          â”‚   â”‚    for the module:                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    Flash-Lite batch receives all    â”‚
                                   â”‚    topic_tags[] across every Q in   â”‚
                                   â”‚    this module â†’ returns frequency  â”‚
                                   â”‚    map â†’ UPSERT high_yield_topics   â”‚
                                   â”‚    (is_high_yield = freq â‰¥ 3)       â”‚
                                   â”‚ 6. Status â†’ indexed                 â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. Data flow â€” query (when student asks a question)

```
Student types / speaks message
        â”‚
        â–¼
  â”Œâ”€â”€â”€ Complexity Router (Flash-Lite, ~200 tokens) â”€â”€â”€â”
  â”‚  Returns: SIMPLE | COMPLEX                        â”‚
  â”‚  COMPLEX â†’ route to DeepSeek R1 (no grounding)   â”‚
  â”‚  SIMPLE  â†’ route to Gemini Flash (+ grounding)   â”‚
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
  â”Œâ”€â”€â”€ Parallel context assembly (all run concurrently) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                                    â”‚
  â”‚  A. Textbook RAG                                                   â”‚
  â”‚       a. Metadata pre-filter (module_tag, subject_tag)             â”‚
  â”‚       b. Dense (pgvector HNSW top-20) + Sparse (FTS top-20)       â”‚
  â”‚       c. RRF merge (0.7 dense / 0.3 sparse; 0.5/0.5 in Tutor)    â”‚
  â”‚       d. Cross-encoder re-rank â†’ top-7                             â”‚
  â”‚       e. MMR diversity filter (Î»=0.7) â†’ top-5                     â”‚
  â”‚       f. Contextual compression (Flash-Lite, parent sections)      â”‚
  â”‚       g. Drop below 0.3 threshold â†’ final chunks [T1â€¦Tn]          â”‚
  â”‚                                                                    â”‚
  â”‚  B. Past-paper intel                                               â”‚
  â”‚       a. Pull high_yield_topics for this module (top-5, freq DESC) â”‚
  â”‚       b. If mode = Tutor AND student is weak on a high-yield topic:â”‚
  â”‚          pull top-2 example past-paper questions on that topic     â”‚
  â”‚          â†’ label as [P1], [P2] in prompt                           â”‚
  â”‚       c. Budget: â‰¤ 800 tokens                                     â”‚
  â”‚                                                                    â”‚
  â”‚  C. Student memory                                                 â”‚
  â”‚       a. Embed query â†’ HNSW search student_memory top-5           â”‚
  â”‚       b. Sort: strong first, weak after, updated_at DESC           â”‚
  â”‚       c. Overlay upcoming viva / timetable context (next 7 days)  â”‚
  â”‚       d. Budget: â‰¤ 1 000 tokens                                   â”‚
  â”‚                                                                    â”‚
  â”‚  D. Short-term memory                                              â”‚
  â”‚       Last 10 messages from this session (chat_messages)           â”‚
  â”‚       Budget: â‰¤ 1 500 tokens                                      â”‚
  â”‚                                                                    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
  â”Œâ”€â”€â”€ Prompt assembly â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  System prompt:                                                    â”‚
  â”‚    â€¢ persona (Chat peer tone / Tutor guide tone)                   â”‚
  â”‚    â€¢ learning_style + explanation_depth (from onboarding)          â”‚
  â”‚    â€¢ [STUDENT CONTEXT] block (memory C)                            â”‚
  â”‚    â€¢ [HIGH-YIELD] block (past-paper B)                             â”‚
  â”‚    â€¢ citation rules: [Tn] textbook, [Wn] web, [Pn] past paper    â”‚
  â”‚  User turn:                                                        â”‚
  â”‚    â€¢ short-term history (D) + current message                      â”‚
  â”‚  Context:                                                          â”‚
  â”‚    â€¢ textbook chunks (A)                                           â”‚
  â”‚    â€¢ past-paper examples (B, if any)                               â”‚
  â”‚  Tool:                                                             â”‚
  â”‚    â€¢ google_search (Tier 1 / Flash only)                           â”‚
  â”‚  Total: â‰¤ 8 000 tokens                                            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
           Stream chosen model via SSE â†’ client
                               â”‚
                               â–¼
  â”Œâ”€â”€â”€ On completion (async) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  1. Persist assistant message + citations â†’ chat_messages          â”‚
  â”‚  2. Update ai_tutor_usage counter for today                        â”‚
  â”‚  3. If session idle 10 min â†’ trigger memory distillation job       â”‚
  â”‚     (Flash-Lite batch: extract struggles / understood â†’ UPSERT    â”‚
  â”‚      student_memory)                                               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 8. Pro personalisation layer

Free tier gives every student a tutor that knows the textbooks, searches the web, and remembers their struggles. Pro (PKR 3 000 / yr) upgrades that into an **adaptive learning engine**. Every feature below is gated behind `users.is_pro = true` and runs on existing signals â€” no new data collection required.

### 8.1 Feature map

| # | Feature | What it does | Data it uses | Model / where it runs |
|---|---|---|---|---|
| P1 | **Spaced-repetition scheduler** | Surfaces the right topic at the right time so retention sticks. Implements a simplified SM-2 variant: each subtopic gets an interval that stretches on correct recall and resets on failure. | `student_memory` (mcq_accuracy, viva_score per subtopic), `progress_matrix.overall_mastery` | Pure SQL + Supabase trigger. No LLM needed â€” the interval math is deterministic. |
| P2 | **Adaptive difficulty** | MCQ and Tutor drill questions ramp difficulty based on live performance. Easy â†’ medium â†’ hard as the student gets them right; drops back one tier on two consecutive misses. | `mcq_attempts.is_correct` (last 5), `mcq_questions.difficulty`, `student_memory` strength | Flash-Lite batch: after every 5 MCQs, emit `EASY | MEDIUM | HARD` for next round. Stored in `student_memory` as `signal_type = 'difficulty_level'`. |
| P3 | **Weekly study-plan generation** | Every Sunday night, a personalised 5-day study plan lands in the student's dashboard. It balances: topics due for spaced repetition, upcoming vivas / exams, high-yield gaps (weak on a high-yield topic = top priority), and learning-style preferences. | `student_memory` (all signals), `high_yield_topics`, `viva_schedules`, `timetable_entries`, `progress_matrix` | Flash (not Lite â€” this is a reasoning-heavy plan). Runs once / week per Pro user via a scheduled Edge Function. Output stored in `study_plans` table. |
| P4 | **Exam-readiness score** | A single 0â€“100 score per module that tells the student "you are X % ready for this exam." Combines: MCQ accuracy on subtopics, viva scores, spaced-repetition completion rate, and high-yield coverage. Displayed on the dashboard module card. | `mcq_statistics`, `viva_bot_sessions`, `progress_matrix`, `high_yield_topics`, spaced-rep completion | Computed as a weighted formula (no LLM). Updated via Supabase trigger on every signal change. Formula in Â§8.2. |
| P5 | **Deep-dive mode** | When a student is stuck, they can tap "Go deep" on any topic. The tutor switches to Tier 2 (DeepSeek R1) automatically and runs a structured Socratic drill: asks guiding questions, never gives the answer directly, forces the student to derive it. Session ends with a summary of what they learned. | Same as standard tutor + short-term memory | DeepSeek R1 (Tier 2). Prompt includes Socratic constraints. Free users see this as a locked feature; Pro users get unlimited sessions. |
| P6 | **Progress narrative** | Once a week, a short paragraph (3â€“4 sentences) summarises the student's learning journey: what improved, what needs attention, encouragement. Displayed in the dashboard. | `student_memory` diffs (this week vs last week), `progress_matrix` deltas | Flash-Lite batch, weekly, same scheduled job as P3. |

### 8.2 Exam-readiness formula

```
readiness = (
    0.35 Ã— avg_mcq_accuracy_on_high_yield_subtopics
  + 0.25 Ã— avg_viva_score_on_high_yield_subtopics
  + 0.20 Ã— spaced_rep_completion_rate          -- % of due items completed this week
  + 0.15 Ã— high_yield_coverage                 -- % of high-yield subtopics touched â‰¥ once
  + 0.05 Ã— attendance_percentage_this_module
) Ã— 100
```

All terms are 0â€“1 before multiplication. Score is clamped 0â€“100. Weights intentionally favour MCQ accuracy and viva â€” those are the closest proxies to actual exam performance. Attendance is a minor nudge (students who skip class tend to score lower, but it's not causal).

### 8.3 Spaced-repetition interval logic (SM-2 lite)

```
On correct answer (quality â‰¥ 3 / 5):
  if interval_index == 0: next_interval = 1 day
  if interval_index == 1: next_interval = 3 days
  else:                   next_interval = prev_interval Ã— easiness_factor

  easiness_factor = max(1.3, EF + (0.1 - (5 - quality) Ã— (0.08 + (5 - quality) Ã— 0.02)))
  interval_index += 1

On wrong answer (quality < 3):
  interval_index = 0
  next_interval = 1 day
  easiness_factor = max(1.3, EF - 0.2)

Storage: one row per (user_id, subtopic_id) in student_memory
         signal_type = 'spaced_rep'
         value = next review timestamp (unix epoch)
         metadata jsonb stores { interval_index, easiness_factor }
```

### 8.4 What's free vs Pro â€” the boundary

| Capability | Free | Pro |
|---|---|---|
| AI Tutor (Chat + Tutor mode) | 4 msgs / day | Unlimited |
| High-yield topic highlighting in tutor responses | âœ“ (same for everyone â€” it's a retrieval feature) | âœ“ |
| Past-paper example questions in drill | âœ“ (2 per session) | Unlimited |
| Spaced-repetition scheduler (P1) | âœ— | âœ“ |
| Adaptive difficulty (P2) | âœ— | âœ“ |
| Weekly study plan (P3) | âœ— | âœ“ |
| Exam-readiness score (P4) | âœ— | âœ“ |
| Deep-dive Socratic mode (P5) | âœ— | âœ“ |
| Progress narrative (P6) | âœ— | âœ“ |
| Viva Bot | âœ— | 180 min / mo |

### 8.5 Cost â€” Pro features add-on

The Pro features are almost free to run because they reuse signals already collected:

| Feature | Extra AI calls / day (at 125 Pro DAU = 25 % of 500) | Extra cost / day |
|---|---|---|
| P2 adaptive difficulty (Flash-Lite, per 5 MCQs) | ~50 | $0.001 |
| P3 weekly study plan (Flash, once / user / week) | ~18 | $0.015 |
| P5 deep-dive (R1, ~2 sessions / day across all Pro users) | ~2 | $0.003 |
| P6 progress narrative (Flash-Lite, weekly) | ~18 | $0.001 |
| P1 + P4 (pure SQL, no LLM) | 0 | $0.000 |
| **Pro add-on total** | | **$0.02 / day â‰ˆ $0.60 / month** |

Pro revenue at 125 subscribers Ã— PKR 3 000 / yr = PKR 375 000 / yr. AI cost for Pro features: ~PKR 1 800 / yr. Margin: **99.5 %**.

---

## 9. Migration path â€” what to build in Phase 3 + Phase 4

### Phase 3 â€” Core AI tutor (Days 17â€“24)

| Day | Work item |
|---|---|
| 17 | Write migration `00002_knowledge.sql` â€” `knowledge_sources`, `knowledge_sections`, `knowledge_chunks`, `past_paper_questions`, `high_yield_topics`, `chat_sessions`, `chat_messages` + all indexes + RLS. Deploy. |
| 18 | Build **textbook ingestion pipeline**: PDF upload â†’ section split â†’ chunk â†’ normalise terms â†’ embed â†’ Q&A extract (Flash-Lite) â†’ insert. Test with 1 textbook. |
| 19 | Build **past-paper ingestion pipeline**: PDF upload â†’ Flash-Lite question extraction â†’ embed questions â†’ insert `past_paper_questions` + mirror as `knowledge_chunks` (type: past_paper) â†’ recalculate `high_yield_topics`. Test with 1 past paper. |
| 20 | Build **hybrid retrieval function**: dense + FTS + RRF + cross-encoder re-rank + MMR + contextual compression + relevance threshold. Unit-test recall on 50 sample questions from the seeded textbook. |
| 21 | Build **chat Route Handler**: complexity router â†’ parallel context assembly (textbook RAG + student memory + past-paper intel + short-term) â†’ prompt assembly â†’ Gemini / R1 stream via SSE. |
| 22 | Build **chat UI**: streaming message bubbles, inline citations ([T], [W], [P]), session list, Chat vs Tutor mode toggle. |
| 23 | Wire **memory distillation**: post-session idle detection â†’ Flash-Lite extract struggles/understood â†’ UPSERT `student_memory` â†’ decay-on-improvement. Integration test. |
| 24 | Seed 2â€“3 textbooks + 1 past paper via admin dashboard. Full end-to-end smoke test: ask a question, verify citations, verify high-yield block appears, verify memory persists across sessions. |

### Phase 4 â€” Pro personalisation (Days 25â€“30)

| Day | Work item |
|---|---|
| 25 | Write migration `00003_student_memory.sql` â€” `student_memory` table + indexes + RLS + Supabase triggers (mcq_attempts â†’ UPSERT signal, viva_bot_session completion â†’ UPSERT signal). |
| 26 | Build **spaced-repetition engine** (P1): SM-2 lite logic as a Supabase function. Build the "due today" query that surfaces subtopics whose `next_review` timestamp has passed. Wire into dashboard. |
| 27 | Build **adaptive difficulty** (P2): Flash-Lite classifier after every 5 MCQs â†’ emit difficulty level â†’ store in `student_memory`. Wire into MCQ Solver so next round respects the level. |
| 28 | Build **exam-readiness score** (P4): computed column via trigger using the Â§8.2 formula. Display on module cards in dashboard. Build **weekly study plan** (P3): scheduled Edge Function (Sunday 23:00 PKT) â†’ Flash generates plan â†’ store in `study_plans` table â†’ display on dashboard. |
| 29 | Build **Deep-dive / Socratic mode** (P5): Pro-gated route handler that forces Tier 2 (R1) + Socratic system prompt. End-of-session summary. Build **progress narrative** (P6): same weekly job as P3, Flash-Lite paragraph â†’ store + display. |
| 30 | Pro paywall UI: gate all P1â€“P6 features behind `is_pro` check. Show "upgrade" CTA on locked features. Smoke test full Pro flow end-to-end. |

---

## 10. Sources consulted

- [Gemini API â€” Embeddings](https://ai.google.dev/gemini-api/docs/embeddings)
- [Gemini Embedding: Powering RAG and context engineering](https://developers.googleblog.com/gemini-embedding-powering-rag-context-engineering/)
- [Grounding with Google Search â€” Gemini API docs](https://ai.google.dev/gemini-api/docs/google-search)
- [Gemini API and AI Studio now offer Grounding with Google Search](https://developers.googleblog.com/en/gemini-api-and-ai-studio-now-offer-grounding-with-google-search/)
- [pgvector vs Pinecone: cost and performance â€” Supabase blog](https://supabase.com/blog/pgvector-vs-pinecone)
- [File Search Tool â€” Google Blog](https://blog.google/innovation-and-ai/technology/developers-tools/file-search-gemini-api/)
- [Embedding Models in 2026 â€” AIMultiple](https://research.aimultiple.com/embedding-models/)
- [Best RAG architectures for medical education â€” MDPI / Springer surveys](https://www.mdpi.com/2076-3417/16/2/963)
- [Gemini Guided Learning Mode â€” Tech & Learning](https://www.techlearning.com/how-to/geminis-guided-learning-mode-from-google-ai-what-educators-need-to-know)
- [Mastering the Gemini Ecosystem 2026 â€” Medium](https://medium.com/@kuntal-c/mastering-the-gemini-ecosystem-a-2026-guide-to-production-grade-ai-agents-53cc79130cab)
- [SM-2 Spaced Repetition Algorithm](https://en.wikipedia.org/wiki/SuperMemo#Algorithm_SM-2)
- [Adaptive difficulty in educational AI â€” Khan Academy eng blog](https://engineering.khanacademy.org/)
- Skills consulted: `rag-engineer`, `ai-engineer`, `rag-implementation`, `hybrid-search-implementation`, `embedding-strategies`, `ai-product`, `llm-app-patterns`, `memory-systems`
